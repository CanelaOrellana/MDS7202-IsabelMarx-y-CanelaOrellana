{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"c8551454eb494a0ea2504c565baae658","deepnote_cell_type":"markdown","id":"XUZ1dFPHzAHl"},"source":["<h1><center>Laboratorio 11: MLOps 🚀</center></h1>\n","\n","<center><strong>MDS7202: Laboratorio de Programación Científica para Ciencia de Datos</strong></center>"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"b45ff1f2b70e461a8c14fcdffc0288d3","deepnote_cell_type":"markdown","id":"UD8X1uhGzAHq"},"source":["### Cuerpo Docente:\n","\n","- Profesor: Pablo Badilla y Ignacio Meza D.\n","- Auxiliar: Sebastián Tinoco\n","- Ayudante: Felipe Arias y Diego Cortez"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"c8d572bf86e34b018a242287c7f2e8e7","deepnote_cell_type":"markdown","id":"tXflExjqzAHr"},"source":["### Equipo: SUPER IMPORTANTE - notebooks sin nombre no serán revisados\n","\n","- Nombre de alumno 1: Isabel Marx\n","- Nombre de alumno 2: Canela Orellana\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"7710a7eee29a47e6b14c12906b413565","deepnote_cell_type":"markdown","id":"AD-V0bbZzAHr","owner_user_id":"badcc427-fd3d-4615-9296-faa43ec69cfb"},"source":["### **Link de repositorio de GitHub:** https://github.com/CanelaOrellana/MDS7202-IsabelMarx-y-CanelaOrellana"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"63299ffd74484a89a624793c177d1b8f","deepnote_cell_type":"markdown","id":"6uBLPj1PzAHs"},"source":["## Temas a tratar\n","\n","- Construcción de Pipelines usando `kedro`\n","- Entrenamiento y comparativa de modelos usando `MLflow`\n","- Formateo de código usando `flake8`, `isort` y `black`\n","\n","## Reglas:\n","\n","- **Grupos de 2 personas**\n","- Asistencia **obligatoria** a instrucciones del lab (viernes 16.15). Luego, pueden quedarse trabajando en las salas o irse.\n","- **No se revisarán entregas de personas ausentes**. \n","- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n","- Prohibidas las copias. \n","- Pueden usar cualquer matrial del curso que estimen conveniente.\n","\n","### Objetivos principales del laboratorio\n","\n","- Automatizar el flujo de vida de un proyecto de machine learning\n","- Registrar y comparar modelos a través de `MLflow`"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"f744e9295c714524856553c93d68a40b","deepnote_cell_type":"markdown"},"source":["# Breve Contexto\n","\n","Es el año 2160 y la industria del turismo espacial está en auge. A nivel mundial, miles de compañías de transbordadores espaciales llevan a turistas a la Luna y los traen de vuelta. Has logrado obtener datos que enumeran las comodidades ofrecidas en cada transbordador espacial, las reseñas de los clientes y la información de las compañías.\n","\n","**Requerimiento**: Construir un modelo que prediga el precio para cada viaje a la Luna y el correspondiente vuelo de regreso."]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"81bee5e5662c4fb4950d3b98df29233e","deepnote_cell_type":"markdown"},"source":["# 0. Requerimientos previos\n","\n","En este laboratorio aprenderemos a trabajar con diversas herramientas que nos facilitarán el trabajo a la hora de desarrollar nuestros proyectos. Comencemos con una de las más básicas y conocidas a nivel profesional: **VSCode**.\n","\n","## VSCode\n","\n","### ¿Qué es VSCode?\n","\n","**Visual Studio Code (VSCode)** es un editor de código fuente gratuito, de código abierto y altamente personalizable, ampliamente utilizado en ciencia de datos. Con una interfaz intuitiva y ligera, ofrece soporte para diversos lenguajes de programación, destacando por su extensibilidad mediante numerosas extensiones que añaden funcionalidades específicas. Además, integra herramientas de control de versiones, depuración y pruebas, facilitando el trabajo en equipo y el desarrollo de proyectos de ciencia de datos de manera eficiente y personalizada.\n","\n","### **¿Porqué se recomienda utilizar VSCode?**\n","\n","- **Versatilidad**: VSCode ofrece soporte para múltiples lenguajes de programación utilizados en ciencia de datos, como Python, R, Julia y muchos más. Esto permite a los científicos de datos trabajar con diferentes tecnologías en un solo entorno integrado.\n","- **Amplia gama de extensiones**: La comunidad de desarrolladores ha creado numerosas extensiones específicas para ciencia de datos en VSCode. Estas extensiones proporcionan características como resaltado de sintaxis, autocompletado inteligente, visualización de datos, integración con bibliotecas populares de aprendizaje automático y mucho más. Estas extensiones ayudan a mejorar la productividad y acelerar el flujo de trabajo en ciencia de datos.\n","- **Interfaz intuitiva y personalizable**: VSCode cuenta con una interfaz de usuario intuitiva y fácil de usar, lo que facilita su adopción por parte de científicos de datos de todos los niveles de experiencia. Además, ofrece una amplia gama de opciones de personalización, lo que permite adaptar el entorno según las preferencias y necesidades individuales.\n","- **Herramientas integradas**: VSCode proporciona herramientas integradas para la depuración de código, ejecución de pruebas y gestión de control de versiones, como Git. Estas características son esenciales para el desarrollo de proyectos de ciencia de datos, ya que facilitan la detección de errores, la optimización del código y el trabajo en equipo.\n","- **Comunidad activa y soporte continuo**: VSCode tiene una comunidad de usuarios y desarrolladores muy activa. Esto significa que hay una gran cantidad de recursos, documentación, tutoriales y foros de discusión disponibles para ayudar a los científicos de datos a aprovechar al máximo la plataforma. Además, Microsoft brinda soporte continuo y lanza actualizaciones frecuentes para mejorar la experiencia de los usuarios.\n","\n","### **¿Cómo instalar VSCode?**\n","\n","Instalar VSCode es directo: simplemente deben dirigirse a la <a href=\"https://code.visualstudio.com/download\">página oficial</a> y descargar la distribución acorde a su máquina local. \n","\n","Una vez hayamos instalado **VSCode** pueden acceder a éste mediante el acceso directo o a través de la terminal:\n","\n","```\n","code .\n","```\n","\n","lo que abrirá **VSCode** en su escritorio root.\n","\n","### **Instalando plugins**\n","\n","Como se mencionó, **VSCode** habilita la instalación de diferentes *plugins* que nos permiten aumentar el potencial de uso de esta herramienta. Quizás el plugin más importante para este laboratorio (y en general, para cualquier desarrollador de Python) es el plugin `Python` el cual nos ayuda a la hora de escribir código mediante features como el interpreter, debugger, entre muchas otras.\n","\n","Veamos cómo instalarlo!\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/ug1DBRF_MjIAAAAC/bill-oreilly-well-do-it-live.gif\" width=\"400\">\n","</p>"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"e6b91feccdf4494e8791eaf6c5311bf0","deepnote_cell_type":"markdown"},"source":["## Ambiente Conda\n","\n","Antes de trabajar, es necesario que instalen `kedro` en sus máquinas. Para eso usaremos los ambientes virtuales de `anaconda`, la cual pueden descargar desde este <a href=\"https://docs.anaconda.com/free/anaconda/install/index.html\">enlace</a>. Una vez tengan instalado `anaconda` en sus máquinas, pueden crear un ambiente virtual ejecutando desde la terminal:\n","\n","```\n","conda create --name kedro-environment python=3.10 -y\n","```\n","\n","El siguiente paso será activar el ambiente recién creado:\n","\n","```\n","conda activate kedro-environment\n","```\n","\n","Con esto activarán el nuevo ambiente, dejando atrás el ambiente principal con el que estaban trabajando. Si quieren volver al ambiente principal de sus máquinas, pueden simplemente ejecutar `conda deactivate`.\n","\n","Una vez dentro del ambiente creado, pueden instalar `kedro` desde `pip`:\n","\n","```\n","pip install kedro\n","```\n","\n","También instalaremos `kedro-viz`, la cual nos ayudará a visualizar el flujo de los datos de nuestro proyecto:\n","\n","```\n","pip install kedro-viz\n","```\n","\n","Por último, pueden verificar que la instalación de `kedro` fue exitosa mediante:\n","\n","```\n","kedro info\n","```\n","\n","Donde deberían ver un output de este estilo:\n","\n","<p align=\"center\">\n","  <img src=\"https://docs.kedro.org/en/stable/_images/kedro_graphic.png\" width=\"350\">\n","</p>\n","\n","Felicidades!! Han instalado `kedro` en sus máquinas :D ahora sí, manos a la obra con el lab!"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"50dc04428a2e455abcdd6340999dc1e7","deepnote_cell_type":"markdown"},"source":["# 2. Crear un nuevo proyecto\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/rvNN3fvmjnAAAAAd/corgi-cute.gif\" width=\"400\">\n","</p>\n","\n","El primer paso de cualquier trabajo con `kedro` es la creación de un nuevo proyecto. Para esto, podemos simplemente ejecutar en la terminal:\n","\n","```\n","kedro new\n","```\n","\n","Donde luego deberemos especificar el nombre del proyecto. \n","\n","La ejecución del comando anterior genera una carpeta con el nombre de su proyecto en su directorio *root*. Tómese un tiempo para abrir esta carpeta y entender su estructura. ¿Qué elementos identifica?\n","\n","Con el proyecto creado, nos moveremos a la carpeta del proyecto para trabajar con sus archivos:\n","\n","```\n","cd nombre_proyecto\n","```\n","\n","El siguiente paso fundamental es instalar las dependencias del proyecto. Primero debemos agregar las siguientes lineas a `src/requirements.txt`:\n","\n","```\n","kedro-datasets[pandas.CSVDataSet, pandas.ExcelDataSet, pandas.ParquetDataSet]~=1.0\n","scikit-learn~=1.0\n","```\n","\n","Luego instalamos las dependencias ejecutando:\n","\n","```\n","pip install -r src/requirements.txt\n","```\n","\n","Genial! Ahora contamos con todas las librerías necesarias para montar nuestro proyecto en `kedro`. Pasemos ahora a cargar los datos!"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"d59dabc2ed3043c68e0e57378e1a7fca","deepnote_cell_type":"markdown"},"source":["# 3. Cargar datos\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/DHkIdy0a-UkAAAAC/loading-cat.gif\" width=\"400\">\n","</p>\n","\n","### **Estructura de datos**\n","\n","Durante un proyecto de ciencia de datos, es común encontrarse con diversas transformaciones de los datos para alcanzar el objetivo deseado. Con el fin de mantener la trazabilidad de los datos en cada transformación, `kedro` divide estas etapas en las siguientes categorías:\n","\n","```\n","├── data\n","│   ├── 01_raw            <-- Datos sin procesar e inmutables\n","│   ├── 02_intermediate   <-- Datos con formato definido\n","│   ├── 03_primary        <-- Datos en el dominio del modelo\n","│   ├── 04_feature        <-- Features del modelo\n","│   ├── 05_model_input    <-- A menudo denominado 'tablas principales'\n","│   ├── 06_models         <-- Modelos serializados\n","│   ├── 07_model_output   <-- Datos generados por las ejecuciones del modelo\n","│   ├── 08_reporting      <-- Cortes descriptivos ad hoc\n","```\n","\n","Estas categorías se encuentran dentro de la carpeta `./data`. El objetivo es almacenar cada versión de los datos en la categoría correspondiente, lo que nos permite mantener la trazabilidad de los datos desde la entrada hasta la salida. Al seguir esta estructura, podemos acceder fácilmente a cada etapa del proceso y rastrear los cambios realizados en los datos a lo largo del proyecto de ciencia de datos.\n","\n","Genial!! Veamos ahora de qué forma podemos registrar cada transformación en su respectiva etapa.\n","\n","### **Catálogo de datos**\n","\n","\n","El **catálogo de datos** en `kedro` es un registro que contiene todas las fuentes de datos utilizadas por el proyecto para administrar la carga y el almacenamiento de datos. Se mapean los nombres de las entradas y salidas de los nodos como claves en un `DataCatalog`, una clase de `kedro` que puede adaptarse a diferentes tipos de almacenamiento de datos. El archivo de configuración de este catálogo se encuentra en la ruta **`conf/base/catalog.yml`**.\n","\n","Para cargar datos en `kedro`, es necesario completar este archivo con la información necesaria para que `kedro` pueda cargar los conjuntos de datos de manera efectiva. Aunque existen argumentos adicionales que pueden optimizar la carga, los argumentos básicos que se deben especificar son el **directorio** y el **tipo** de los datos de la siguiente manera:\n","\n","```yaml\n","dataset_1:\n","    type: tipo_de_archivo\n","    filepath: ruta_del_archivo\n","\n","dataset_2:\n","    type: tipo_de_archivo\n","    filepath: ruta_del_archivo\n","...\n","```\n","\n","\n","Por ejemplo:\n","\n","```yaml\n","ventas: \n","  type: pandas.JSONDataSet\n","  filepath: data/01_raw/ventas.json\n","```\n","\n","Este <a href=\"https://docs.kedro.org/en/stable/kedro_datasets.html\">enlace</a> contiene la lista completa de conectores que acepta `kedro` para la lectura de datos y así llenar el campo `type`.\n","\n","Para verificar que los datos se están cargando de manera exitosa, podemos testear esto a través de una sesión de `kedro ipython`:\n","\n","```\n","kedro ipython\n","```\n","\n","Donde podemos probar a cargar y printear los datos del dataset:\n","\n","```\n","dataset_1 = catalog.load(\"dataset_1\")\n","dataset_1.head()\n","exit()\n","```\n","\n","Listo! Ahora que conocemos como `kedro` maneja los datos pasemos ahora a construir los `pipelines` de nuestro proyecto."]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"11599e62454e471e94fd74e922d35439","deepnote_cell_type":"markdown"},"source":["# 4. Creación de pipelines\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/e5TDUiAGEowAAAAC/pipes-bursting.gif\" width=\"400\">\n","</p>\n","\n","Sabiendo como cargar los datos, buscaremos ahora automatizar las diferentes etapas de un sistema basado en machine learning por medio de `pipelines`. En específico, buscaremos automatizar los siguientes procesos:\n","\n","- **Lectura y pre procesamiento de datos**\n","- **Holdout y entrenamiento del modelo**\n","\n","Comenzaremos creando un pipeline para cada etapa:\n","\n","```\n","kedro pipeline create data_prep\n","kedro pipeline create train_model\n","```\n","\n","La ejecución del código anterior genera una carpeta para cada pipeline en la ruta `./src/nombre_proyecto/pipelines/`. Cada carpeta posee 3 archivos:\n","\n","- `nodes.py`: contiene las funciones usadas en el pipeline (transformación de datos, split, etc.)\n","- `pipeline.py`: contiene la generación del pipeline en sí mismo\n","- `__init__.py`: permite que Python pueda importar los archivos necesarios del `Pipeline`\n","\n","Además, cada `pipeline` tiene anexado un archivo `.yml` con los parámetros de la ejecución (como el `random_seed`, `learning rate`, etc) en la ruta `./conf/base/parameters/`.\n","\n","Veamos ahora como completar cada uno de estos archivos!"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"d04ffcb194244351b0d55d90785758cf","deepnote_cell_type":"markdown"},"source":["## 4.1 Preparación de datos (2.5 puntos)\n","\n","Comenzaremos completando la información del archivo `node.py`. La idea es que este archivo contenga todas las funciones necesarias para pre procesar los datos de manera adecuada por medio de código `Python` común y corriente.\n","\n","Por otro lado, el archivo `pipeline.py` debe contener una función `create_pipeline` la cual debe retornar el pipeline invocando las funciones generadas en el archivo `node.py`. La estructura de este archivo debe seguir la siguiente forma:\n","\n","```python\n","from kedro.pipeline import Pipeline, node, pipeline\n","from .nodes import func_1, func_2, ...\n","def create_pipeline(**kwargs) -> Pipeline:\n","    return pipeline(\n","        [\n","            node(\n","                func=func_1,\n","                inputs=\"dataset_1\",\n","                outputs=\"output_1\",\n","                name=\"nombre_nodo1\",\n","            ),\n","            node(\n","                func=func_2,\n","                inputs=\"dataset_2\",\n","                outputs=\"output_2\",\n","                name=\"nombre_nodo2\",\n","            ),\n","            ...\n","        ]\n","    )\n","```\n","\n","Además, es necesario también completar el archivo `./conf/base/catalog.yml` con la información de este `pipeline`. Para esto, simplemente escribimos nuevas lineas debajo de las escritas del punto 3, es decir:\n","\n","```yaml\n","dataset_1:\n","  type: file_type\n","  filepath: file_path\n","\n","dataset_2:\n","  type: file_type\n","  filepath: file_path\n","    \n","...\n","\n","nombre_nodo1:\n","  type: file_type\n","  filepath: file_path (path de salida)\n","\n","nombre_nodo2:\n","  type: file_type\n","  filepath: file_path (path de salida)\n","  \n","...\n","```\n","\n","A medida que los proyectos se van haciendo más grandes, los nodos pueden tender a ser más complejos. Una buena forma de probar que los nodos funcionan con normalidad es a través de la ejecución del siguiente comando:\n","\n","```\n","kedro run --nodes=nombre_nodo\n","```\n","\n","o simplemente testear la totalidad del pipeline a través de:\n","\n","```\n","kedro run\n","```\n","\n","Finalmente, es posible obtener una ilustración del flujo de nuestro proyecto a través de:\n","\n","```\n","kedro viz\n","```\n","\n","Lo que abrirá una ventana en nuestro navegador con un gráfico interactivo de nuestro proyecto.\n","\n","En consideración de todo lo anterior y usando la carpeta del pipeline `data_processing`, se le pide:\n","\n","1. Completar el archivo `node.py` usando como base el archivo `prep_functions.py` proporcionado. Debe además completar la función `get_data` donde debe leer los archivos `companies.csv`, `shuttles.xlsx` y `reviews.csv` **directamente** desde el siguiente <a href=\"https://github.com/MDS7202/lab_11\">repositorio</a> para luego retornarlos como salida de su función (**importante: no debe guardar los archivos con métodos como `.to_csv` o similares**).\n","2. Completar el archivo `pipeline.py` usando las funciones generadas en `node.py`. El pipeline debe consistir de 4 pasos: \n","    - Leer los datos `companies.csv`, `shuttles.xlsx` y `reviews.csv` desde el repositorio github usando la función `get_data`\n","    - Procesar `companies.csv` usando la función `preprocess_companies`\n","    - Procesar `shuttles.xlsx` usando la función `preprocess_shuttles`\n","    - Procesar `reviews.csv` y la salida de los 2 pasos anteriores usando la función `create_model_input_table`\n","3. Completar el archivo `catalog.yml` con la información del `pipeline` generado. En específico, se requiere que:\n","    - Los datos del primer paso deben sean almacenados en `./data/01_raw`\n","    - Los archivos generados tras procesar `companies.csv` y `shuttles.xlsx` deben ser guardados en `./data/02_intermediate/`\n","    - El archivo generado tras procesar `reviews.csv` y las 2 salidas anteriores debe ser guardado en `./data/03_primary/`\n","    - Todos estos archivos deben ser guardados en formato `parquet`\n","\n","*Hint: Se recomienda sólo completar la función `get_data` del archivo `prep_functions.py`, deje todo el resto sin cambios*\n","\n","*Hint 2: Puede especificar más de un input/output por medio de listas, por ejemplo: `inputs=[\"dataset_1\", \"dataset_2\"]`*\n","\n","*Hint 3: También es posible especificar input/output como vacíos, por ejemplo: `outputs=None`*\n","\n","**Visualización esperada de ejecutar `kedro viz`:**\n","\n","<img src=\"https://raw.githubusercontent.com/MDS7202/lab_11/main/pipe_1.png\"/>"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Content was found.\n"]}],"source":["# requests.get() funcionaaa. esto está en la función get_data() en nodes.py\n","import base64\n","import requests\n","\n","companies_url = 'https://raw.githubusercontent.com/MDS7202/lab_11/main/companies.csv'\n","shuttles_url = 'https://raw.githubusercontent.com/MDS7202/lab_11/main/shuttles.xlsx'\n","reviews_url = 'https://raw.githubusercontent.com/MDS7202/lab_11/main/reviews.csv'\n","companies_req = requests.get(companies_url)\n","c = 0\n","if companies_req.status_code == requests.codes.ok:\n","    c += 1\n","shuttles_req = requests.get(shuttles_url)\n","if shuttles_req.status_code == requests.codes.ok and c == 1:\n","    c += 1\n","reviews_req = requests.get(reviews_url)\n","if reviews_req.status_code == requests.codes.ok and c == 2:\n","    print('Content was found.')\n","else:\n","    print('Content was not found.')"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["# probando que estén wenos los url\n","import pandas as pd\n","a  = pd.read_csv(reviews_url)#, encoding='utf-8')#, encoding = 'unicode_escape')"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"ec29870b7bae41378061c6ad6050335d","deepnote_cell_type":"markdown"},"source":["## 4.2 Entrenar modelo (3 puntos)\n","\n","Llegó la hora de la verdad!! En esta sección intentaremos usar `Kedro` para automatizar el entrenamiento de nuestros modelos y `MLflow` para registrar registrarlos y acceder a una interfaz gráfica donde podremos generar comparativas entre ellos.\n","\n","En primer lugar instalaremos algunas librerías que utilizaremos en este apartado:"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"a82f50232bee4a49b084752a03ef22ed","deepnote_cell_type":"markdown"},"source":["```python\n","!pip install xgboost\n","!pip install lightgbm\n","!pip install mlflow\n","```"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"6c1a8cc548c6424d932a25f352b9c544","deepnote_cell_type":"markdown"},"source":["Ya habíamos hablado de `Kedro` y sobre su funcionamiento, pero aún no hemos hablado nada sobre `MLflow`. \n","\n","En ese sentido cabe preguntarse: **¡¿Qué !\"#@ es `MLflow`?!**\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/eusgDKT4smQAAAAC/matthew-perry-chandler-bing.gif\" width=\"400\">\n","</p>\n","\n","## MLflow\n","\n","`MLflow` es una plataforma de código abierto que simplifica la gestión y seguimiento de proyectos de aprendizaje automático. Con sus herramientas, los desarrolladores pueden organizar, rastrear y comparar experimentos, además de registrar modelos y controlar versiones. \n","\n","<p align=\"center\">\n","  <img src=\"https://spark.apache.org/images/mlflow-logo.png\" width=\"350\">\n","</p>\n","\n","Si bien esta plataforma cuenta con un gran número de herramientas y funcionalidades, en este laboratorio trabajaremos con dos:\n","1. **Runs**: Registro que constituye la información guardada tras la ejecución de un entrenamiento. Cada `run` tiene su propio run_id, el cual sirve como identificador para el entrenamiento en sí mismo. Dentro de cada `run` podremos acceder a información como los hiperparámetros utilizados, las métricas obtenidas, las librerías requeridas y hasta nos permite descargar el modelo entrenado.\n","2. **Experiments**: Se utilizan para agrupar y organizar diferentes ejecuciones de modelos (`runs`). En ese sentido, un experimento puede agrupar 1 o más `runs`. De esta manera, es posible también registrar métricas, parámetros y archivos (artefactos) asociados a cada experimento.\n","\n","### **Todo bien pero entonces, ¿cómo se usa en la práctica `MLflow`?**\n","\n","Es sencillo! Considerando un problema de machine learning genérico, podemos registrar la información relevante del entrenamiento ejecutando `mlflow.autolog()` antes entrenar nuestro modelo. Veamos este bonito ejemplo facilitado por los mismos creadores de `MLflow`:\n","\n","```python\n","import mlflow # importar mlflow\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import load_diabetes\n","from sklearn.ensemble import RandomForestRegressor\n","\n","db = load_diabetes()\n","X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n","\n","# Create and train models.\n","rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n","\n","mlflow.autolog() # registrar automáticamente información del entrenamiento\n","with mlflow.start_run(): # delimita inicio y fin del run\n","    # aquí comienza el run\n","    rf.fit(X_train, y_train) # train the model\n","    predictions = rf.predict(X_test) # Use the model to make predictions on the test dataset.\n","    # aquí termina el run\n","```\n","\n","Si ustedes ejecutan el código anterior en sus máquinas locales (desde un jupyter notebook por ejemplo) se darán cuenta que en su directorio root se ha creado la carpeta `mlruns`. Esta carpeta lleva el tracking de todos los entrenamientos ejecutados desde el directorio root (importante: si se cambian de directorio y vuelven a ejecutar el código anterior, se creará otra carpeta y no tendrán acceso al entrenamiento anterior). Para visualizar estos entrenamientos, `MLflow` nos facilita hermosa interfaz visual a la que podemos acceder ejecutando:\n","\n","```\n","mlflow ui\n","```\n","\n","y luego pinchando en la ruta http://127.0.0.1:5000 que nos retorna la terminal. Veamos en vivo algunas de sus funcionalidades!\n","\n","<p align=\"center\">\n","  <img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExZXVuM3A5MW1heDFpa21qbGlwN2pyc2VoNnZsMmRzODZxdnluemo2bCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3o84sq21TxDH6PyYms/giphy.gif\" width=\"400\">\n","</p>\n","\n","Les dejamos también algunos comandos útiles:\n","\n","- `mlflow.create_experiment(\"nombre_experimento\")`: Les permite crear un nuevo experimento para agrupar entrenamientos\n","- `mlflow.log_metric(\"nombre_métrica\", métrica)`: Les permite registrar una métrica *custom* bajo el nombre de \"nombre_métrica\"\n","\n","## Combinando Kedro + MLflow\n","\n","Ahora que tenemos conocimiento de ambas herramientas, intentemos ahora combinarlas para **más sabor**. El objetivo de este apartado es simple: automatizar el entrenamiento de nuestros modelos usando `Kedro`, registrando de forma automática los resultados de cada entrenamiento en `MLflow`. Para esto, deberemos volver a completar los archivos `node.py`, `pipeline.py` y `catalog.yml` de forma correspondiente. \n","\n","Considerando el objetivo planteado y usando la carpeta del pipeline `train_model`, se le pide:\n","1. Completar el archivo `./conf/base/parameters/train_model.yml` especificando los siguientes campos:\n","```yaml\n","split_params:\n","  target: \"price\"\n","  train_ratio: 0.8\n","  valid_ratio: 0.1\n","  random_state: 67\n","```\n","2. Completar el archivo `node.py` usando como base el archivo `train_functions.py`. Se le pide además completar la función `train_model`, la cual debe:\n","    - Recibir como entrada los conjuntos de entrenamiento y validación\n","    - Entrenar los modelos `LinearRegression`, `RandomForestRegressor`, `SVR`, `XGBRegressor` y `LGBMRegressor` con sus parámetros por defecto\n","    - Registrar cada entrenamiento en `MLflow` en un **experimento nuevo** por cada ejecución del pipeline y registrar la métrica `mean absolute error` con el nombre de `\"valid_mae\"` (se descontará puntaje si todos los experimentos quedan guardados en *Default*). Además, cada `experiment` y `run` deben ser guardados con un **nombre interpretable**, **fácilmente reconocible** y **distinto al entregado por defecto** (ejemplo para run: \"XGBoost con lr 0.1\").\n","    - Devolver el mejor modelo usando la función `get_best_model` (contenida en el archivo `train_functions.py`)\n","3. Completar el archivo `pipeline.py` usando las funciones generadas en `node.py`. El pipeline debe consistir de 3 pasos:\n","    - Dividir los datos usando la función `split_data` usando los parámetros especificados en `./conf/base/parameters/train_model.yml`\n","    - Entrenar los modelos y retornar el modelo con mejor `mean_absolute_error` en el conjunto de validación usando `train_model`\n","    - Evaluar el modelo entrenado en el conjunto de test usando la función `evaluate_model` (contenida en el archivo `train_functions.py`)\n","4. Completar el archivo `catalog.yml` especificando que:\n","    - Las salidas de la función `split_data` sean guardadas en la carpeta `./data/05_model_input/` en formato `parquet`.\n","    - La salida de la función `train_model` sea guardada en la carpeta `./data/06_models/` en formato `pickle`\n","\n","\n","*Hint: Los parámetros pueden ser usados como input por medio del string `\"params:split_params\"`*\n","\n","*Hint 2: Le puede ser útil revisar los parámetros que recibe `mlflow.start_run`*\n","\n","**Visualización esperada de ejecutar `kedro viz`:**\n","\n","<img src=\"https://raw.githubusercontent.com/MDS7202/lab_11/main/pipe_2.png\" />"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"18229829e8cb49ab9799835fe287cc3e","deepnote_cell_type":"markdown"},"source":["# 5. Cierre del proyecto\n","\n","¡Felicitaciones por llegar hasta aquí en el laboratorio! Han explorado y aprendido sobre diferentes herramientas para automatizar de manera eficiente y profesional el flujo de vida de un proyecto de machine learning. Antes de exportar nuestro proyecto de Kedro, intentemos mejorar el formato de nuestro código utilizando **linters** y **formatters** para aplicar las mejores prácticas en nuestro proyecto. Pero primero, ¿qué son los linters y formatters?\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/ijfGyd3aBg0AAAAC/wrap-it-up-watch.gif\" width=\"400\">\n","</p>\n","\n","## Linter\n","\n","Un linter es una herramienta que ayuda a identificar y corregir problemas en el código fuente de un programa. Su objetivo principal es analizar el código en busca de posibles errores, inconsistencias o malas prácticas, y proporcionar recomendaciones para mejorar la calidad del código.\n","\n","El linter realiza un análisis estático del código, lo que significa que examina el código sin ejecutarlo y aplica reglas predefinidas o personalizadas para verificar su cumplimiento con estándares de codificación, convenciones de estilo y buenas prácticas. Esto incluye aspectos como el formato del código, el uso adecuado de variables y nombres de funciones, y la detección de posibles errores o vulnerabilidades, entre otros.\n","\n","Algunos linters también ofrecen funcionalidades adicionales, como la corrección automática de errores o la generación de informes detallados sobre las áreas problemáticas del código. Esto ayuda a los programadores a mantener un código limpio, legible y de alta calidad, y a reducir los errores antes de la ejecución del programa.\n","\n","Ejemplo de linter: `flake8`\n","\n","## Formatter\n","\n","Un formatter (formateador) es una herramienta que ayuda a establecer y mantener una estructura y estilo consistentes en el código fuente. Su función principal es aplicar automáticamente reglas de formato y convenciones de estilo predefinidas para asegurarse de que el código esté correctamente organizado y presentado.\n","\n","El formatter realiza cambios en el formato del código, como la indentación, la colocación de espacios, la alineación y la organización de elementos. A diferencia de un linter, que se enfoca en identificar errores y malas prácticas, el formatter se centra en la estética y el formato del código, mejorando su legibilidad y uniformidad.\n","\n","La ventaja de utilizar un formatter es que garantiza que todo el código en un proyecto siga las mismas reglas de estilo, independientemente de quién lo haya escrito. Esto facilita la colaboración y la comprensión del código, especialmente en equipos de desarrollo donde varios programadores contribuyen al mismo proyecto.\n","\n","Ejemplos de formatter: `black` y `isort`\n","\n","## Formateando nuestro proyecto (0.5 puntos)\n","\n","Habiendo conocido el funcionamiento básico de los **linter** y **formatter**, buscaremos aplicar estas herramientas a nuestro proyecto y así mejorar el formato de nuestro código. Comenzaremos instalando `flake8`, `black` y `isort` sobre nuestras máquinas:\n","\n","```\n","pip install flake8\n","pip install black\n","pip install isort\n","```\n","\n","Con las librerías instaladas, comenzaremos ejecutando `flake8` sobre el directorio root de nuestro proyecto:\n","\n","```\n","flake8 .\n","```\n","\n","Como ya se explicó un poco más arriba, `flake8` es un **linter** que nos ayuda a detectar posibles incongruencias en nuestro código. En caso de encontrar cualquier detalle, `flake8` levanta una alerta y le señala a usted la linea que debe corregir. En ese sentido, si usted se encuentra con cualquier tipo de alerta deben corregir las líneas indicadas y volver a ejecutar `flake8` sobre su proyecto hasta que no se encuentren más alertas (**esto se evaluará en la entrega**).\n","\n","Asimismo, podemos ejecutar `black` y `isort` de la misma manera:\n","\n","```\n","black .\n","isort .\n","```\n","\n","Si bien ambos paquetes pertenecen a la categoría **formatter**, estos cumplen diferentes funciones: El objetivo de `black` es formatear el código según el <a href=\"https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html\">estilo black</a> y de esta manera eliminar líneas vacías, segmentar líneas de código que se extiendan sobre un largo específico, remplazar las comillas simples por dobles, entre muchas otras cosas. Por otro lado, el objetivo de `isort` es ordenar las librerías importadas de manera alfabética y seccionadas por tipo.\n","\n","\n","## Exportar proyecto\n","\n","Finalmente!! Después de tanto trabajo, llegó la hora de exportar nuestro proyecto. Si bien existen <a href=\"https://docs.kedro.org/en/stable/tutorial/package_a_project.html#package-a-kedro-project\">otras formas</a> de exportar su proyecto, esta vez revisaremos la totalidad del proyecto por lo que se les pide que compriman todo su proyecto en un archivo `.zip` y lo suban a U-cursos."]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"8518a0d6d4a44deb96d7645d7f3fd472","deepnote_cell_type":"markdown"},"source":["# Conclusión\n","\n","Eso ha sido todo para el lab de hoy, esperamos de corazón que les haya gustado el lab y que por sobre todo hayan aprendido un poco sobre el fascinante mundo de **MLOps**. Recuerden que el laboratorio tiene un plazo de entrega de una semana y que ante cualquier duda del laboratorio, no duden en contactarnos por mail o U-cursos.\n","\n","<p align=\"center\">\n","  <img src=\"https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/0a4e90b8-a7c8-4019-90f1-b9eb16a6fe6b/d7i06j2-209054b9-be1e-46b0-aa61-ffbe4dc1ebda.gif?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzBhNGU5MGI4LWE3YzgtNDAxOS05MGYxLWI5ZWIxNmE2ZmU2YlwvZDdpMDZqMi0yMDkwNTRiOS1iZTFlLTQ2YjAtYWE2MS1mZmJlNGRjMWViZGEuZ2lmIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.Qt1_4hixKd8mTuRub4aksuPW1ZIDK-r7X6Rhh5lnqtI\" width=\"400\">\n","</p>"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"bd0a30a982fa4e1cb4f0a6769d183cab","deepnote_cell_type":"markdown"},"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"colab":{"collapsed_sections":["LCOUC4jss148","GtG74Cphq56p"],"name":"Laboratorio4.ipynb","provenance":[],"toc_visible":true},"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"c7876e26c2d44f5b87d905e285a5b38c","deepnote_persisted_session":{"createdAt":"2023-06-16T02:51:36.770Z"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Tabla de Contenidos","title_sidebar":"Contenidos","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"241.867px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}
