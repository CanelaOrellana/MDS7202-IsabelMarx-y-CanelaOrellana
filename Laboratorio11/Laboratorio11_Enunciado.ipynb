{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"c8551454eb494a0ea2504c565baae658","deepnote_cell_type":"markdown","id":"XUZ1dFPHzAHl"},"source":["<h1><center>Laboratorio 11: MLOps üöÄ</center></h1>\n","\n","<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos</strong></center>"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"b45ff1f2b70e461a8c14fcdffc0288d3","deepnote_cell_type":"markdown","id":"UD8X1uhGzAHq"},"source":["### Cuerpo Docente:\n","\n","- Profesor: Pablo Badilla y Ignacio Meza D.\n","- Auxiliar: Sebasti√°n Tinoco\n","- Ayudante: Felipe Arias y Diego Cortez"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"c8d572bf86e34b018a242287c7f2e8e7","deepnote_cell_type":"markdown","id":"tXflExjqzAHr"},"source":["### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n","\n","- Nombre de alumno 1: Isabel Marx\n","- Nombre de alumno 2: Canela Orellana\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"7710a7eee29a47e6b14c12906b413565","deepnote_cell_type":"markdown","id":"AD-V0bbZzAHr","owner_user_id":"badcc427-fd3d-4615-9296-faa43ec69cfb"},"source":["### **Link de repositorio de GitHub:** https://github.com/CanelaOrellana/MDS7202-IsabelMarx-y-CanelaOrellana"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"63299ffd74484a89a624793c177d1b8f","deepnote_cell_type":"markdown","id":"6uBLPj1PzAHs"},"source":["## Temas a tratar\n","\n","- Construcci√≥n de Pipelines usando `kedro`\n","- Entrenamiento y comparativa de modelos usando `MLflow`\n","- Formateo de c√≥digo usando `flake8`, `isort` y `black`\n","\n","## Reglas:\n","\n","- **Grupos de 2 personas**\n","- Asistencia **obligatoria** a instrucciones del lab (viernes 16.15). Luego, pueden quedarse trabajando en las salas o irse.\n","- **No se revisar√°n entregas de personas ausentes**. \n","- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n","- Prohibidas las copias. \n","- Pueden usar cualquer matrial del curso que estimen conveniente.\n","\n","### Objetivos principales del laboratorio\n","\n","- Automatizar el flujo de vida de un proyecto de machine learning\n","- Registrar y comparar modelos a trav√©s de `MLflow`"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"f744e9295c714524856553c93d68a40b","deepnote_cell_type":"markdown"},"source":["# Breve Contexto\n","\n","Es el a√±o 2160 y la industria del turismo espacial est√° en auge. A nivel mundial, miles de compa√±√≠as de transbordadores espaciales llevan a turistas a la Luna y los traen de vuelta. Has logrado obtener datos que enumeran las comodidades ofrecidas en cada transbordador espacial, las rese√±as de los clientes y la informaci√≥n de las compa√±√≠as.\n","\n","**Requerimiento**: Construir un modelo que prediga el precio para cada viaje a la Luna y el correspondiente vuelo de regreso."]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"81bee5e5662c4fb4950d3b98df29233e","deepnote_cell_type":"markdown"},"source":["# 0. Requerimientos previos\n","\n","En este laboratorio aprenderemos a trabajar con diversas herramientas que nos facilitar√°n el trabajo a la hora de desarrollar nuestros proyectos. Comencemos con una de las m√°s b√°sicas y conocidas a nivel profesional: **VSCode**.\n","\n","## VSCode\n","\n","### ¬øQu√© es VSCode?\n","\n","**Visual Studio Code (VSCode)** es un editor de c√≥digo fuente gratuito, de c√≥digo abierto y altamente personalizable, ampliamente utilizado en ciencia de datos. Con una interfaz intuitiva y ligera, ofrece soporte para diversos lenguajes de programaci√≥n, destacando por su extensibilidad mediante numerosas extensiones que a√±aden funcionalidades espec√≠ficas. Adem√°s, integra herramientas de control de versiones, depuraci√≥n y pruebas, facilitando el trabajo en equipo y el desarrollo de proyectos de ciencia de datos de manera eficiente y personalizada.\n","\n","### **¬øPorqu√© se recomienda utilizar VSCode?**\n","\n","- **Versatilidad**: VSCode ofrece soporte para m√∫ltiples lenguajes de programaci√≥n utilizados en ciencia de datos, como Python, R, Julia y muchos m√°s. Esto permite a los cient√≠ficos de datos trabajar con diferentes tecnolog√≠as en un solo entorno integrado.\n","- **Amplia gama de extensiones**: La comunidad de desarrolladores ha creado numerosas extensiones espec√≠ficas para ciencia de datos en VSCode. Estas extensiones proporcionan caracter√≠sticas como resaltado de sintaxis, autocompletado inteligente, visualizaci√≥n de datos, integraci√≥n con bibliotecas populares de aprendizaje autom√°tico y mucho m√°s. Estas extensiones ayudan a mejorar la productividad y acelerar el flujo de trabajo en ciencia de datos.\n","- **Interfaz intuitiva y personalizable**: VSCode cuenta con una interfaz de usuario intuitiva y f√°cil de usar, lo que facilita su adopci√≥n por parte de cient√≠ficos de datos de todos los niveles de experiencia. Adem√°s, ofrece una amplia gama de opciones de personalizaci√≥n, lo que permite adaptar el entorno seg√∫n las preferencias y necesidades individuales.\n","- **Herramientas integradas**: VSCode proporciona herramientas integradas para la depuraci√≥n de c√≥digo, ejecuci√≥n de pruebas y gesti√≥n de control de versiones, como Git. Estas caracter√≠sticas son esenciales para el desarrollo de proyectos de ciencia de datos, ya que facilitan la detecci√≥n de errores, la optimizaci√≥n del c√≥digo y el trabajo en equipo.\n","- **Comunidad activa y soporte continuo**: VSCode tiene una comunidad de usuarios y desarrolladores muy activa. Esto significa que hay una gran cantidad de recursos, documentaci√≥n, tutoriales y foros de discusi√≥n disponibles para ayudar a los cient√≠ficos de datos a aprovechar al m√°ximo la plataforma. Adem√°s, Microsoft brinda soporte continuo y lanza actualizaciones frecuentes para mejorar la experiencia de los usuarios.\n","\n","### **¬øC√≥mo instalar VSCode?**\n","\n","Instalar VSCode es directo: simplemente deben dirigirse a la <a href=\"https://code.visualstudio.com/download\">p√°gina oficial</a> y descargar la distribuci√≥n acorde a su m√°quina local. \n","\n","Una vez hayamos instalado **VSCode** pueden acceder a √©ste mediante el acceso directo o a trav√©s de la terminal:\n","\n","```\n","code .\n","```\n","\n","lo que abrir√° **VSCode** en su escritorio root.\n","\n","### **Instalando plugins**\n","\n","Como se mencion√≥, **VSCode** habilita la instalaci√≥n de diferentes *plugins* que nos permiten aumentar el potencial de uso de esta herramienta. Quiz√°s el plugin m√°s importante para este laboratorio (y en general, para cualquier desarrollador de Python) es el plugin `Python` el cual nos ayuda a la hora de escribir c√≥digo mediante features como el interpreter, debugger, entre muchas otras.\n","\n","Veamos c√≥mo instalarlo!\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/ug1DBRF_MjIAAAAC/bill-oreilly-well-do-it-live.gif\" width=\"400\">\n","</p>"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"e6b91feccdf4494e8791eaf6c5311bf0","deepnote_cell_type":"markdown"},"source":["## Ambiente Conda\n","\n","Antes de trabajar, es necesario que instalen `kedro` en sus m√°quinas. Para eso usaremos los ambientes virtuales de `anaconda`, la cual pueden descargar desde este <a href=\"https://docs.anaconda.com/free/anaconda/install/index.html\">enlace</a>. Una vez tengan instalado `anaconda` en sus m√°quinas, pueden crear un ambiente virtual ejecutando desde la terminal:\n","\n","```\n","conda create --name kedro-environment python=3.10 -y\n","```\n","\n","El siguiente paso ser√° activar el ambiente reci√©n creado:\n","\n","```\n","conda activate kedro-environment\n","```\n","\n","Con esto activar√°n el nuevo ambiente, dejando atr√°s el ambiente principal con el que estaban trabajando. Si quieren volver al ambiente principal de sus m√°quinas, pueden simplemente ejecutar `conda deactivate`.\n","\n","Una vez dentro del ambiente creado, pueden instalar `kedro` desde `pip`:\n","\n","```\n","pip install kedro\n","```\n","\n","Tambi√©n instalaremos `kedro-viz`, la cual nos ayudar√° a visualizar el flujo de los datos de nuestro proyecto:\n","\n","```\n","pip install kedro-viz\n","```\n","\n","Por √∫ltimo, pueden verificar que la instalaci√≥n de `kedro` fue exitosa mediante:\n","\n","```\n","kedro info\n","```\n","\n","Donde deber√≠an ver un output de este estilo:\n","\n","<p align=\"center\">\n","  <img src=\"https://docs.kedro.org/en/stable/_images/kedro_graphic.png\" width=\"350\">\n","</p>\n","\n","Felicidades!! Han instalado `kedro` en sus m√°quinas :D ahora s√≠, manos a la obra con el lab!"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"50dc04428a2e455abcdd6340999dc1e7","deepnote_cell_type":"markdown"},"source":["# 2. Crear un nuevo proyecto\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/rvNN3fvmjnAAAAAd/corgi-cute.gif\" width=\"400\">\n","</p>\n","\n","El primer paso de cualquier trabajo con `kedro` es la creaci√≥n de un nuevo proyecto. Para esto, podemos simplemente ejecutar en la terminal:\n","\n","```\n","kedro new\n","```\n","\n","Donde luego deberemos especificar el nombre del proyecto. \n","\n","La ejecuci√≥n del comando anterior genera una carpeta con el nombre de su proyecto en su directorio *root*. T√≥mese un tiempo para abrir esta carpeta y entender su estructura. ¬øQu√© elementos identifica?\n","\n","Con el proyecto creado, nos moveremos a la carpeta del proyecto para trabajar con sus archivos:\n","\n","```\n","cd nombre_proyecto\n","```\n","\n","El siguiente paso fundamental es instalar las dependencias del proyecto. Primero debemos agregar las siguientes lineas a `src/requirements.txt`:\n","\n","```\n","kedro-datasets[pandas.CSVDataSet, pandas.ExcelDataSet, pandas.ParquetDataSet]~=1.0\n","scikit-learn~=1.0\n","```\n","\n","Luego instalamos las dependencias ejecutando:\n","\n","```\n","pip install -r src/requirements.txt\n","```\n","\n","Genial! Ahora contamos con todas las librer√≠as necesarias para montar nuestro proyecto en `kedro`. Pasemos ahora a cargar los datos!"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"d59dabc2ed3043c68e0e57378e1a7fca","deepnote_cell_type":"markdown"},"source":["# 3. Cargar datos\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/DHkIdy0a-UkAAAAC/loading-cat.gif\" width=\"400\">\n","</p>\n","\n","### **Estructura de datos**\n","\n","Durante un proyecto de ciencia de datos, es com√∫n encontrarse con diversas transformaciones de los datos para alcanzar el objetivo deseado. Con el fin de mantener la trazabilidad de los datos en cada transformaci√≥n, `kedro` divide estas etapas en las siguientes categor√≠as:\n","\n","```\n","‚îú‚îÄ‚îÄ data\n","‚îÇ   ‚îú‚îÄ‚îÄ 01_raw            <-- Datos sin procesar e inmutables\n","‚îÇ   ‚îú‚îÄ‚îÄ 02_intermediate   <-- Datos con formato definido\n","‚îÇ   ‚îú‚îÄ‚îÄ 03_primary        <-- Datos en el dominio del modelo\n","‚îÇ   ‚îú‚îÄ‚îÄ 04_feature        <-- Features del modelo\n","‚îÇ   ‚îú‚îÄ‚îÄ 05_model_input    <-- A menudo denominado 'tablas principales'\n","‚îÇ   ‚îú‚îÄ‚îÄ 06_models         <-- Modelos serializados\n","‚îÇ   ‚îú‚îÄ‚îÄ 07_model_output   <-- Datos generados por las ejecuciones del modelo\n","‚îÇ   ‚îú‚îÄ‚îÄ 08_reporting      <-- Cortes descriptivos ad hoc\n","```\n","\n","Estas categor√≠as se encuentran dentro de la carpeta `./data`. El objetivo es almacenar cada versi√≥n de los datos en la categor√≠a correspondiente, lo que nos permite mantener la trazabilidad de los datos desde la entrada hasta la salida. Al seguir esta estructura, podemos acceder f√°cilmente a cada etapa del proceso y rastrear los cambios realizados en los datos a lo largo del proyecto de ciencia de datos.\n","\n","Genial!! Veamos ahora de qu√© forma podemos registrar cada transformaci√≥n en su respectiva etapa.\n","\n","### **Cat√°logo de datos**\n","\n","\n","El **cat√°logo de datos** en `kedro` es un registro que contiene todas las fuentes de datos utilizadas por el proyecto para administrar la carga y el almacenamiento de datos. Se mapean los nombres de las entradas y salidas de los nodos como claves en un `DataCatalog`, una clase de `kedro` que puede adaptarse a diferentes tipos de almacenamiento de datos. El archivo de configuraci√≥n de este cat√°logo se encuentra en la ruta **`conf/base/catalog.yml`**.\n","\n","Para cargar datos en `kedro`, es necesario completar este archivo con la informaci√≥n necesaria para que `kedro` pueda cargar los conjuntos de datos de manera efectiva. Aunque existen argumentos adicionales que pueden optimizar la carga, los argumentos b√°sicos que se deben especificar son el **directorio** y el **tipo** de los datos de la siguiente manera:\n","\n","```yaml\n","dataset_1:\n","    type: tipo_de_archivo\n","    filepath: ruta_del_archivo\n","\n","dataset_2:\n","    type: tipo_de_archivo\n","    filepath: ruta_del_archivo\n","...\n","```\n","\n","\n","Por ejemplo:\n","\n","```yaml\n","ventas: \n","  type: pandas.JSONDataSet\n","  filepath: data/01_raw/ventas.json\n","```\n","\n","Este <a href=\"https://docs.kedro.org/en/stable/kedro_datasets.html\">enlace</a> contiene la lista completa de conectores que acepta `kedro` para la lectura de datos y as√≠ llenar el campo `type`.\n","\n","Para verificar que los datos se est√°n cargando de manera exitosa, podemos testear esto a trav√©s de una sesi√≥n de `kedro ipython`:\n","\n","```\n","kedro ipython\n","```\n","\n","Donde podemos probar a cargar y printear los datos del dataset:\n","\n","```\n","dataset_1 = catalog.load(\"dataset_1\")\n","dataset_1.head()\n","exit()\n","```\n","\n","Listo! Ahora que conocemos como `kedro` maneja los datos pasemos ahora a construir los `pipelines` de nuestro proyecto."]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"11599e62454e471e94fd74e922d35439","deepnote_cell_type":"markdown"},"source":["# 4. Creaci√≥n de pipelines\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/e5TDUiAGEowAAAAC/pipes-bursting.gif\" width=\"400\">\n","</p>\n","\n","Sabiendo como cargar los datos, buscaremos ahora automatizar las diferentes etapas de un sistema basado en machine learning por medio de `pipelines`. En espec√≠fico, buscaremos automatizar los siguientes procesos:\n","\n","- **Lectura y pre procesamiento de datos**\n","- **Holdout y entrenamiento del modelo**\n","\n","Comenzaremos creando un pipeline para cada etapa:\n","\n","```\n","kedro pipeline create data_prep\n","kedro pipeline create train_model\n","```\n","\n","La ejecuci√≥n del c√≥digo anterior genera una carpeta para cada pipeline en la ruta `./src/nombre_proyecto/pipelines/`. Cada carpeta posee 3 archivos:\n","\n","- `nodes.py`: contiene las funciones usadas en el pipeline (transformaci√≥n de datos, split, etc.)\n","- `pipeline.py`: contiene la generaci√≥n del pipeline en s√≠ mismo\n","- `__init__.py`: permite que Python pueda importar los archivos necesarios del `Pipeline`\n","\n","Adem√°s, cada `pipeline` tiene anexado un archivo `.yml` con los par√°metros de la ejecuci√≥n (como el `random_seed`, `learning rate`, etc) en la ruta `./conf/base/parameters/`.\n","\n","Veamos ahora como completar cada uno de estos archivos!"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"d04ffcb194244351b0d55d90785758cf","deepnote_cell_type":"markdown"},"source":["## 4.1 Preparaci√≥n de datos (2.5 puntos)\n","\n","Comenzaremos completando la informaci√≥n del archivo `node.py`. La idea es que este archivo contenga todas las funciones necesarias para pre procesar los datos de manera adecuada por medio de c√≥digo `Python` com√∫n y corriente.\n","\n","Por otro lado, el archivo `pipeline.py` debe contener una funci√≥n `create_pipeline` la cual debe retornar el pipeline invocando las funciones generadas en el archivo `node.py`. La estructura de este archivo debe seguir la siguiente forma:\n","\n","```python\n","from kedro.pipeline import Pipeline, node, pipeline\n","from .nodes import func_1, func_2, ...\n","def create_pipeline(**kwargs) -> Pipeline:\n","    return pipeline(\n","        [\n","            node(\n","                func=func_1,\n","                inputs=\"dataset_1\",\n","                outputs=\"output_1\",\n","                name=\"nombre_nodo1\",\n","            ),\n","            node(\n","                func=func_2,\n","                inputs=\"dataset_2\",\n","                outputs=\"output_2\",\n","                name=\"nombre_nodo2\",\n","            ),\n","            ...\n","        ]\n","    )\n","```\n","\n","Adem√°s, es necesario tambi√©n completar el archivo `./conf/base/catalog.yml` con la informaci√≥n de este `pipeline`. Para esto, simplemente escribimos nuevas lineas debajo de las escritas del punto 3, es decir:\n","\n","```yaml\n","dataset_1:\n","  type: file_type\n","  filepath: file_path\n","\n","dataset_2:\n","  type: file_type\n","  filepath: file_path\n","    \n","...\n","\n","nombre_nodo1:\n","  type: file_type\n","  filepath: file_path (path de salida)\n","\n","nombre_nodo2:\n","  type: file_type\n","  filepath: file_path (path de salida)\n","  \n","...\n","```\n","\n","A medida que los proyectos se van haciendo m√°s grandes, los nodos pueden tender a ser m√°s complejos. Una buena forma de probar que los nodos funcionan con normalidad es a trav√©s de la ejecuci√≥n del siguiente comando:\n","\n","```\n","kedro run --nodes=nombre_nodo\n","```\n","\n","o simplemente testear la totalidad del pipeline a trav√©s de:\n","\n","```\n","kedro run\n","```\n","\n","Finalmente, es posible obtener una ilustraci√≥n del flujo de nuestro proyecto a trav√©s de:\n","\n","```\n","kedro viz\n","```\n","\n","Lo que abrir√° una ventana en nuestro navegador con un gr√°fico interactivo de nuestro proyecto.\n","\n","En consideraci√≥n de todo lo anterior y usando la carpeta del pipeline `data_processing`, se le pide:\n","\n","1. Completar el archivo `node.py` usando como base el archivo `prep_functions.py` proporcionado. Debe adem√°s completar la funci√≥n `get_data` donde debe leer los archivos `companies.csv`, `shuttles.xlsx` y `reviews.csv` **directamente** desde el siguiente <a href=\"https://github.com/MDS7202/lab_11\">repositorio</a> para luego retornarlos como salida de su funci√≥n (**importante: no debe guardar los archivos con m√©todos como `.to_csv` o similares**).\n","2. Completar el archivo `pipeline.py` usando las funciones generadas en `node.py`. El pipeline debe consistir de 4 pasos: \n","    - Leer los datos `companies.csv`, `shuttles.xlsx` y `reviews.csv` desde el repositorio github usando la funci√≥n `get_data`\n","    - Procesar `companies.csv` usando la funci√≥n `preprocess_companies`\n","    - Procesar `shuttles.xlsx` usando la funci√≥n `preprocess_shuttles`\n","    - Procesar `reviews.csv` y la salida de los 2 pasos anteriores usando la funci√≥n `create_model_input_table`\n","3. Completar el archivo `catalog.yml` con la informaci√≥n del `pipeline` generado. En espec√≠fico, se requiere que:\n","    - Los datos del primer paso deben sean almacenados en `./data/01_raw`\n","    - Los archivos generados tras procesar `companies.csv` y `shuttles.xlsx` deben ser guardados en `./data/02_intermediate/`\n","    - El archivo generado tras procesar `reviews.csv` y las 2 salidas anteriores debe ser guardado en `./data/03_primary/`\n","    - Todos estos archivos deben ser guardados en formato `parquet`\n","\n","*Hint: Se recomienda s√≥lo completar la funci√≥n `get_data` del archivo `prep_functions.py`, deje todo el resto sin cambios*\n","\n","*Hint 2: Puede especificar m√°s de un input/output por medio de listas, por ejemplo: `inputs=[\"dataset_1\", \"dataset_2\"]`*\n","\n","*Hint 3: Tambi√©n es posible especificar input/output como vac√≠os, por ejemplo: `outputs=None`*\n","\n","**Visualizaci√≥n esperada de ejecutar `kedro viz`:**\n","\n","<img src=\"https://raw.githubusercontent.com/MDS7202/lab_11/main/pipe_1.png\"/>"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Content was found.\n"]}],"source":["# requests.get() funcionaaa. esto est√° en la funci√≥n get_data() en nodes.py\n","import base64\n","import requests\n","\n","companies_url = 'https://raw.githubusercontent.com/MDS7202/lab_11/main/companies.csv'\n","shuttles_url = 'https://raw.githubusercontent.com/MDS7202/lab_11/main/shuttles.xlsx'\n","reviews_url = 'https://raw.githubusercontent.com/MDS7202/lab_11/main/reviews.csv'\n","companies_req = requests.get(companies_url)\n","c = 0\n","if companies_req.status_code == requests.codes.ok:\n","    c += 1\n","shuttles_req = requests.get(shuttles_url)\n","if shuttles_req.status_code == requests.codes.ok and c == 1:\n","    c += 1\n","reviews_req = requests.get(reviews_url)\n","if reviews_req.status_code == requests.codes.ok and c == 2:\n","    print('Content was found.')\n","else:\n","    print('Content was not found.')"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["# probando que est√©n wenos los url\n","import pandas as pd\n","a  = pd.read_csv(reviews_url)#, encoding='utf-8')#, encoding = 'unicode_escape')"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"ec29870b7bae41378061c6ad6050335d","deepnote_cell_type":"markdown"},"source":["## 4.2 Entrenar modelo (3 puntos)\n","\n","Lleg√≥ la hora de la verdad!! En esta secci√≥n intentaremos usar `Kedro` para automatizar el entrenamiento de nuestros modelos y `MLflow` para registrar registrarlos y acceder a una interfaz gr√°fica donde podremos generar comparativas entre ellos.\n","\n","En primer lugar instalaremos algunas librer√≠as que utilizaremos en este apartado:"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"a82f50232bee4a49b084752a03ef22ed","deepnote_cell_type":"markdown"},"source":["```python\n","!pip install xgboost\n","!pip install lightgbm\n","!pip install mlflow\n","```"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"6c1a8cc548c6424d932a25f352b9c544","deepnote_cell_type":"markdown"},"source":["Ya hab√≠amos hablado de `Kedro` y sobre su funcionamiento, pero a√∫n no hemos hablado nada sobre `MLflow`. \n","\n","En ese sentido cabe preguntarse: **¬°¬øQu√© !\"#@ es `MLflow`?!**\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/eusgDKT4smQAAAAC/matthew-perry-chandler-bing.gif\" width=\"400\">\n","</p>\n","\n","## MLflow\n","\n","`MLflow` es una plataforma de c√≥digo abierto que simplifica la gesti√≥n y seguimiento de proyectos de aprendizaje autom√°tico. Con sus herramientas, los desarrolladores pueden organizar, rastrear y comparar experimentos, adem√°s de registrar modelos y controlar versiones. \n","\n","<p align=\"center\">\n","  <img src=\"https://spark.apache.org/images/mlflow-logo.png\" width=\"350\">\n","</p>\n","\n","Si bien esta plataforma cuenta con un gran n√∫mero de herramientas y funcionalidades, en este laboratorio trabajaremos con dos:\n","1. **Runs**: Registro que constituye la informaci√≥n guardada tras la ejecuci√≥n de un entrenamiento. Cada `run` tiene su propio run_id, el cual sirve como identificador para el entrenamiento en s√≠ mismo. Dentro de cada `run` podremos acceder a informaci√≥n como los hiperpar√°metros utilizados, las m√©tricas obtenidas, las librer√≠as requeridas y hasta nos permite descargar el modelo entrenado.\n","2. **Experiments**: Se utilizan para agrupar y organizar diferentes ejecuciones de modelos (`runs`). En ese sentido, un experimento puede agrupar 1 o m√°s `runs`. De esta manera, es posible tambi√©n registrar m√©tricas, par√°metros y archivos (artefactos) asociados a cada experimento.\n","\n","### **Todo bien pero entonces, ¬øc√≥mo se usa en la pr√°ctica `MLflow`?**\n","\n","Es sencillo! Considerando un problema de machine learning gen√©rico, podemos registrar la informaci√≥n relevante del entrenamiento ejecutando `mlflow.autolog()` antes entrenar nuestro modelo. Veamos este bonito ejemplo facilitado por los mismos creadores de `MLflow`:\n","\n","```python\n","import mlflow # importar mlflow\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import load_diabetes\n","from sklearn.ensemble import RandomForestRegressor\n","\n","db = load_diabetes()\n","X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n","\n","# Create and train models.\n","rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n","\n","mlflow.autolog() # registrar autom√°ticamente informaci√≥n del entrenamiento\n","with mlflow.start_run(): #¬†delimita inicio y fin del run\n","    #¬†aqu√≠ comienza el run\n","    rf.fit(X_train, y_train) # train the model\n","    predictions = rf.predict(X_test) # Use the model to make predictions on the test dataset.\n","    # aqu√≠ termina el run\n","```\n","\n","Si ustedes ejecutan el c√≥digo anterior en sus m√°quinas locales (desde un jupyter notebook por ejemplo) se dar√°n cuenta que en su directorio root se ha creado la carpeta `mlruns`. Esta carpeta lleva el tracking de todos los entrenamientos ejecutados desde el directorio root (importante: si se cambian de directorio y vuelven a ejecutar el c√≥digo anterior, se crear√° otra carpeta y no tendr√°n acceso al entrenamiento anterior). Para visualizar estos entrenamientos, `MLflow` nos facilita hermosa interfaz visual a la que podemos acceder ejecutando:\n","\n","```\n","mlflow ui\n","```\n","\n","y luego pinchando en la ruta http://127.0.0.1:5000 que nos retorna la terminal. Veamos en vivo algunas de sus funcionalidades!\n","\n","<p align=\"center\">\n","  <img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExZXVuM3A5MW1heDFpa21qbGlwN2pyc2VoNnZsMmRzODZxdnluemo2bCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3o84sq21TxDH6PyYms/giphy.gif\" width=\"400\">\n","</p>\n","\n","Les dejamos tambi√©n algunos comandos √∫tiles:\n","\n","- `mlflow.create_experiment(\"nombre_experimento\")`: Les permite crear un nuevo experimento para agrupar entrenamientos\n","- `mlflow.log_metric(\"nombre_m√©trica\", m√©trica)`: Les permite registrar una m√©trica *custom* bajo el nombre de \"nombre_m√©trica\"\n","\n","## Combinando Kedro + MLflow\n","\n","Ahora que tenemos conocimiento de ambas herramientas, intentemos ahora combinarlas para **m√°s sabor**. El objetivo de este apartado es simple: automatizar el entrenamiento de nuestros modelos usando `Kedro`, registrando de forma autom√°tica los resultados de cada entrenamiento en `MLflow`. Para esto, deberemos volver a completar los archivos `node.py`, `pipeline.py` y `catalog.yml` de forma correspondiente. \n","\n","Considerando el objetivo planteado y usando la carpeta del pipeline `train_model`, se le pide:\n","1. Completar el archivo `./conf/base/parameters/train_model.yml` especificando los siguientes campos:\n","```yaml\n","split_params:\n","  target: \"price\"\n","  train_ratio: 0.8\n","  valid_ratio: 0.1\n","  random_state: 67\n","```\n","2. Completar el archivo `node.py` usando como base el archivo `train_functions.py`. Se le pide adem√°s completar la funci√≥n `train_model`, la cual debe:\n","    - Recibir como entrada los conjuntos de entrenamiento y validaci√≥n\n","    - Entrenar los modelos `LinearRegression`, `RandomForestRegressor`, `SVR`, `XGBRegressor` y `LGBMRegressor` con sus par√°metros por defecto\n","    - Registrar cada entrenamiento en `MLflow` en un **experimento nuevo** por cada ejecuci√≥n del pipeline y registrar la m√©trica `mean absolute error` con el nombre de `\"valid_mae\"` (se descontar√° puntaje si todos los experimentos quedan guardados en *Default*). Adem√°s, cada `experiment` y `run` deben ser guardados con un **nombre interpretable**, **f√°cilmente reconocible** y **distinto al entregado por defecto** (ejemplo para run: \"XGBoost con lr 0.1\").\n","    - Devolver el mejor modelo usando la funci√≥n `get_best_model` (contenida en el archivo `train_functions.py`)\n","3. Completar el archivo `pipeline.py` usando las funciones generadas en `node.py`. El pipeline debe consistir de 3 pasos:\n","    - Dividir los datos usando la funci√≥n `split_data` usando los par√°metros especificados en `./conf/base/parameters/train_model.yml`\n","    - Entrenar los modelos y retornar el modelo con mejor `mean_absolute_error` en el conjunto de validaci√≥n usando `train_model`\n","    - Evaluar el modelo entrenado en el conjunto de test usando la funci√≥n `evaluate_model` (contenida en el archivo `train_functions.py`)\n","4. Completar el archivo `catalog.yml` especificando que:\n","    - Las salidas de la funci√≥n `split_data` sean guardadas en la carpeta `./data/05_model_input/` en formato `parquet`.\n","    - La salida de la funci√≥n `train_model` sea guardada en la carpeta `./data/06_models/` en formato `pickle`\n","\n","\n","*Hint: Los par√°metros pueden ser usados como input por medio del string `\"params:split_params\"`*\n","\n","*Hint 2: Le puede ser √∫til revisar los par√°metros que recibe `mlflow.start_run`*\n","\n","**Visualizaci√≥n esperada de ejecutar `kedro viz`:**\n","\n","<img src=\"https://raw.githubusercontent.com/MDS7202/lab_11/main/pipe_2.png\" />"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"18229829e8cb49ab9799835fe287cc3e","deepnote_cell_type":"markdown"},"source":["# 5. Cierre del proyecto\n","\n","¬°Felicitaciones por llegar hasta aqu√≠ en el laboratorio! Han explorado y aprendido sobre diferentes herramientas para automatizar de manera eficiente y profesional el flujo de vida de un proyecto de machine learning. Antes de exportar nuestro proyecto de Kedro, intentemos mejorar el formato de nuestro c√≥digo utilizando **linters** y **formatters** para aplicar las mejores pr√°cticas en nuestro proyecto. Pero primero, ¬øqu√© son los linters y formatters?\n","\n","<p align=\"center\">\n","  <img src=\"https://media.tenor.com/ijfGyd3aBg0AAAAC/wrap-it-up-watch.gif\" width=\"400\">\n","</p>\n","\n","## Linter\n","\n","Un linter es una herramienta que ayuda a identificar y corregir problemas en el c√≥digo fuente de un programa. Su objetivo principal es analizar el c√≥digo en busca de posibles errores, inconsistencias o malas pr√°cticas, y proporcionar recomendaciones para mejorar la calidad del c√≥digo.\n","\n","El linter realiza un an√°lisis est√°tico del c√≥digo, lo que significa que examina el c√≥digo sin ejecutarlo y aplica reglas predefinidas o personalizadas para verificar su cumplimiento con est√°ndares de codificaci√≥n, convenciones de estilo y buenas pr√°cticas. Esto incluye aspectos como el formato del c√≥digo, el uso adecuado de variables y nombres de funciones, y la detecci√≥n de posibles errores o vulnerabilidades, entre otros.\n","\n","Algunos linters tambi√©n ofrecen funcionalidades adicionales, como la correcci√≥n autom√°tica de errores o la generaci√≥n de informes detallados sobre las √°reas problem√°ticas del c√≥digo. Esto ayuda a los programadores a mantener un c√≥digo limpio, legible y de alta calidad, y a reducir los errores antes de la ejecuci√≥n del programa.\n","\n","Ejemplo de linter: `flake8`\n","\n","## Formatter\n","\n","Un formatter (formateador) es una herramienta que ayuda a establecer y mantener una estructura y estilo consistentes en el c√≥digo fuente. Su funci√≥n principal es aplicar autom√°ticamente reglas de formato y convenciones de estilo predefinidas para asegurarse de que el c√≥digo est√© correctamente organizado y presentado.\n","\n","El formatter realiza cambios en el formato del c√≥digo, como la indentaci√≥n, la colocaci√≥n de espacios, la alineaci√≥n y la organizaci√≥n de elementos. A diferencia de un linter, que se enfoca en identificar errores y malas pr√°cticas, el formatter se centra en la est√©tica y el formato del c√≥digo, mejorando su legibilidad y uniformidad.\n","\n","La ventaja de utilizar un formatter es que garantiza que todo el c√≥digo en un proyecto siga las mismas reglas de estilo, independientemente de qui√©n lo haya escrito. Esto facilita la colaboraci√≥n y la comprensi√≥n del c√≥digo, especialmente en equipos de desarrollo donde varios programadores contribuyen al mismo proyecto.\n","\n","Ejemplos de formatter: `black` y `isort`\n","\n","## Formateando nuestro proyecto (0.5 puntos)\n","\n","Habiendo conocido el funcionamiento b√°sico de los **linter** y **formatter**, buscaremos aplicar estas herramientas a nuestro proyecto y as√≠ mejorar el formato de nuestro c√≥digo. Comenzaremos instalando `flake8`, `black` y `isort` sobre nuestras m√°quinas:\n","\n","```\n","pip install flake8\n","pip install black\n","pip install isort\n","```\n","\n","Con las librer√≠as instaladas, comenzaremos ejecutando `flake8` sobre el directorio root de nuestro proyecto:\n","\n","```\n","flake8 .\n","```\n","\n","Como ya se explic√≥ un poco m√°s arriba, `flake8` es un **linter** que nos ayuda a detectar posibles incongruencias en nuestro c√≥digo. En caso de encontrar cualquier detalle, `flake8` levanta una alerta y le se√±ala a usted la linea que debe corregir. En ese sentido, si usted se encuentra con cualquier tipo de alerta deben corregir las l√≠neas indicadas y volver a ejecutar `flake8` sobre su proyecto hasta que no se encuentren m√°s alertas (**esto se evaluar√° en la entrega**).\n","\n","Asimismo, podemos ejecutar `black` y `isort` de la misma manera:\n","\n","```\n","black .\n","isort .\n","```\n","\n","Si bien ambos paquetes pertenecen a la categor√≠a **formatter**, estos cumplen diferentes funciones: El objetivo de `black` es formatear el c√≥digo seg√∫n el <a href=\"https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html\">estilo black</a> y de esta manera eliminar l√≠neas vac√≠as, segmentar l√≠neas de c√≥digo que se extiendan sobre un largo espec√≠fico, remplazar las comillas simples por dobles, entre muchas otras cosas. Por otro lado, el objetivo de `isort` es ordenar las librer√≠as importadas de manera alfab√©tica y seccionadas por tipo.\n","\n","\n","## Exportar proyecto\n","\n","Finalmente!! Despu√©s de tanto trabajo, lleg√≥ la hora de exportar nuestro proyecto. Si bien existen <a href=\"https://docs.kedro.org/en/stable/tutorial/package_a_project.html#package-a-kedro-project\">otras formas</a> de exportar su proyecto, esta vez revisaremos la totalidad del proyecto por lo que se les pide que compriman todo su proyecto en un archivo `.zip` y lo suban a U-cursos."]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"8518a0d6d4a44deb96d7645d7f3fd472","deepnote_cell_type":"markdown"},"source":["# Conclusi√≥n\n","\n","Eso ha sido todo para el lab de hoy, esperamos de coraz√≥n que les haya gustado el lab y que por sobre todo hayan aprendido un poco sobre el fascinante mundo de **MLOps**. Recuerden que el laboratorio tiene un plazo de entrega de una semana y que ante cualquier duda del laboratorio, no duden en contactarnos por mail o U-cursos.\n","\n","<p align=\"center\">\n","  <img src=\"https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/0a4e90b8-a7c8-4019-90f1-b9eb16a6fe6b/d7i06j2-209054b9-be1e-46b0-aa61-ffbe4dc1ebda.gif?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzBhNGU5MGI4LWE3YzgtNDAxOS05MGYxLWI5ZWIxNmE2ZmU2YlwvZDdpMDZqMi0yMDkwNTRiOS1iZTFlLTQ2YjAtYWE2MS1mZmJlNGRjMWViZGEuZ2lmIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.Qt1_4hixKd8mTuRub4aksuPW1ZIDK-r7X6Rhh5lnqtI\" width=\"400\">\n","</p>"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"bd0a30a982fa4e1cb4f0a6769d183cab","deepnote_cell_type":"markdown"},"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"colab":{"collapsed_sections":["LCOUC4jss148","GtG74Cphq56p"],"name":"Laboratorio4.ipynb","provenance":[],"toc_visible":true},"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"c7876e26c2d44f5b87d905e285a5b38c","deepnote_persisted_session":{"createdAt":"2023-06-16T02:51:36.770Z"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Tabla de Contenidos","title_sidebar":"Contenidos","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"241.867px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}
