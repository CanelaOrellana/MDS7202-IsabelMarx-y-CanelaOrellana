{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"b9ead8046fc74db1bf0eb99287c3c91e","deepnote_cell_type":"markdown"},"source":"![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)\n"},{"cell_type":"markdown","metadata":{"cell_id":"ca18b36a3bd54ad09e518c5b63b08ef6","deepnote_cell_type":"markdown"},"source":"# Proyecto: Riesgo en el Banco Giturra\n\n**MDS7202: Laboratorio de Programación Científica para Ciencia de Datos**\n\n### Cuerpo Docente:\n\n- Profesor: Pablo Badilla, Ignacio Meza De La Jara\n- Auxiliar: Sebastián Tinoco\n- Ayudante: Diego Cortez M., Felipe Arias T.\n\n_Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir._\n\n---\n\n## Reglas\n\n- Fecha de entrega: 01/06/2021\n- **Grupos de 2 personas.**\n- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n- Estrictamente prohibida la copia.\n- Pueden usar cualquier material del curso que estimen conveniente.\n"},{"cell_type":"markdown","metadata":{"cell_id":"20bff171d2954732b74d8f532c21895a","deepnote_cell_type":"markdown"},"source":"---\n"},{"cell_type":"markdown","metadata":{"cell_id":"7292d5dfddb9408d8999b7c0e63fc55c","deepnote_cell_type":"markdown"},"source":"# Presentación del Problema\n"},{"cell_type":"markdown","metadata":{"cell_id":"4e407b3b55de4930a31e10cf4e25f51c","deepnote_cell_type":"markdown"},"source":"![](https://www.diarioeldia.cl/u/fotografias/fotosnoticias/2019/11/8/67218.jpg)\n"},{"cell_type":"markdown","metadata":{"cell_id":"57d46ee3f60b457fa3932bf4ac15a828","deepnote_cell_type":"markdown"},"source":"**Giturra**, un banquero astuto y ambicioso, estableció su propio banco con el objetivo de obtener enormes ganancias. Sin embargo, su reputación se vio empañada debido a las tasas de interés usureras que imponía a sus clientes. A medida que su banco crecía, Giturra enfrentaba una creciente cantidad de préstamos impagados, lo que amenazaba su negocio y su prestigio.\n\nPara abordar este desafío, Giturra reconoció la necesidad de reducir los riesgos de préstamo y mejorar la calidad de los préstamos otorgados. Decidió aprovechar la ciencia de datos y el análisis de riesgo crediticio. Contrató a un equipo de expertos para desarrollar un modelo predictivo de riesgo crediticio.\n\nCabe señalar que lo modelos solicitados por el banquero deben ser interpretables. Ya que estos le permitira al equipo comprender y explicar cómo se toman las decisiones crediticias. Utilizando visualizaciones claras y explicaciones detalladas, pudieron identificar las características más relevantes, le permitirá analizar la distribución de la importancia de las variables y evaluar si los modelos son coherentes con el negocio.\n\nPara esto Giturra les solicita crear un modelo de riesgo disponibilizandoles una amplia gama de variables de sus usuarios: como historiales de crédito, ingresos y otros factores financieros relevantes, para evaluar la probabilidad de incumplimiento de pago de los clientes. Con esta información, Giturra podra tomar decisiones más informadas en cuanto a los préstamos, ofreciendo condiciones más favorables a aquellos con menor riesgo de impago.\n"},{"cell_type":"markdown","metadata":{"cell_id":"731884b83ce840b1b52d26a180626317","deepnote_cell_type":"markdown"},"source":"## Instalación de Librerías y Carga de Datos.\n"},{"cell_type":"markdown","metadata":{"cell_id":"0c0b7aa1618543519231311b567d9bc3","deepnote_cell_type":"markdown"},"source":"Para el desarrollo de su proyecto, utilice el conjunto de datos `dataset.pq` para entrenar un modelo de su elección. Además, se adjunta junto con los datos del proyecto un archivo llamado `requirements.txt` que contiene todas las bibliotecas y versiones necesarias para el desarrollo del proyecto. Se le recomienda levantar un ambiente de `conda` para instalar estas librerías y así evitar cualquier problema con las versiones.\n"},{"cell_type":"markdown","metadata":{"cell_id":"8c769111397b49e980ce325911903144","deepnote_cell_type":"markdown"},"source":"---\n\n## Secciones Requeridas en el Informe\n\nLa siguiente lista detalla las secciones que debe contener su notebook para resolver el proyecto. \nEs importante que al momento de desarrollar cada una de las secciones, estas sean escritas en un formato tipo **informe**, donde describan detalladamente cada uno de los puntos realizados.\n\n### 1. Introducción [0.5 puntos]\n\n_Esta sección es literalmente una muy breve introducción con todo lo necesario para entender que hicieron en su proyecto._\n\n- Describir brevemente el problema planteado (¿Qué se intenta predecir?)\n- Describir brevemente los datos de entrada que les provee el problema.\n- Describir las métricas que utilizarán para evaluar los modelos generados. Eligan **una métrica** adecuada para el desarrollo del proyecto **según la tarea que deben resolver y la institución a la cuál será su contraparte** y luego justifiquen su elección. Considerando que los datos presentan desbalanceo y que el uso de la métrica 'accuracy' sería incorrecto, enfoquen su elección en una de las métricas precision, recall o f1-score y en la clase que será evaluada.\n- [Escribir al final] Describir brevemente el modelo que usaron para resolver el problema (incluyendo las transformaciones intermedias de datos).\n- [Escribir al final] Indicar si lograron resolver el problema a través de su modelo. Indiquen además si creen que los resultados de su mejor modelo son aceptables y como les fue con respecto al resto de los equipos.\n\n### 2. Carga de datos Análisis Exploratorio de Datos [Sin puntaje]\n\n_La idea de esta sección es que cargen y exploren el dataset para así obtener una idea de como son los datos y como se relacionan con el problema._\n\nCargue los datos y realice un análisis exploratorio de datos para investigar patrones, tendencias y relaciones en un conjunto de datos. Se adjuntan diversos scripts para abodar rápidamente este punto. La descripción de las columnas las pueden encontrar en el siguiente [enlace](https://www.kaggle.com/datasets/parisrohan/credit-score-classification).\n\n**NO deben escribir nada**, solo ejecutar el código y encontrar los patrones con los cuales se basaran para generar el modelo.\n\n### 3. Preparación de Datos [0.5 puntos]\n\n_Esta sección consiste en generar los distintos pasos para preparar sus datos con el fin de luego poder crear su modelo._\n\n#### 3.1 Preprocesamiento con `ColumnTransformer`\n\n- Convierta las columnas mal leidas a sus tipos correspondientes (float, str, etc...)\n- Genere un `ColumnTransformer` que:\n  - Preprocese datos categóricos y ordinales.\n  - Escale/estandarice datos numéricos.\n  - Uitlice `.set_output(transform=\"pandas\")` sobre su `ColumnTransformer` para setear el formato de salida a de las transformaciones a pandas.\n\n- Luego, pruebe las transformaciones utilizando `fit_transform`.\n\n- Posteriormente, ejecute un Holdout que le permita más adelante evaluar los modelos.\n\n#### 3.2 Holdout \n\nEjecute `train_test_split` para generar un conjunto de entrenamiento y de prueba. \n\nSi bien tienen la libertad de generar conjuntos de validación para robustecer sus resultados, este no es requisito obligatorio y no se le asignará puntaje por esto (esto debido a que grid-search ocupa internamente cross validation).\n\n#### 3.3 Datos nulos.\n\nComo habrá visto, existe la posibilidad de que algunos datos sean nulos. En esta sección se le solicita justificar, previo a comenzar el modelado, decidir si conservar e imputar los datos nulos o eliminar las filas. \n\nNote que la decisión que tomen aquí puede afectar fuertemente el rendimiento de los modelos. \nY como siempre, más adelante tienen el espacio para experimentar con ambas opciones.\n\n#### 3.4 Feature Engineering [Bonus - 0.5 puntos]\n\nEn esta sección, se espera que apliquen su conocimiento y creatividad para identificar y construir características que brinden una mejor orientación a su modelo para identificar los casos deseados. Para motivar la construcción de nuevas características, se recomienda explorar las siguientes posibilidades:\n\n- Generar ratios que relacionen variables categóricas con numéricas. Estos ratios permiten capturar relaciones proporcionales o comparativas entre diferentes categorías y valores numéricos.\n- Combinación de rankings entre variables numéricas y categóricas.\n- Discretización de variables numéricas a categóricas.\n- Etc...\n\n**Importantes**: Al explorar estas posibilidades no se limiten solo a estas propuestas, pueden aplicar otras técnicas de feature engineering pertinentes para mejorar la capacidad de su modelo para comprender y aprovechar los patrones presentes en los datos. \n\n### 4. Baseline [1.5 puntos]\n\n_En esta sección deben crear los modelos más básicos posibles que resuelvan el problema dado. La idea de estos modelos son usarlos como comparación para que en el siguiente paso lo puedan mejorar._\n\nImplemente, entrene y evalúe varias `Pipeline` enfocadas en resolver el problema de clasificación en donde la diferencia entre estas sea el modelo utilizado.\n\n\nPara esto, cada Pipeline debe:\n\n- Tener el `ColumnTransformer` implementado en la sección anterior como primer paso.\n- Implementar un imputador en caso de haber decidido conservar los datos nulos.\n- Implementar un clasificador en la salida (ver siguiente lista).\n  \nY además, \n- Ser evaluado de forma general imprimiendo un `classification_report`.\n- Calcular y guardar la métrica seleccionada en el punto 1.2 en un arreglo de métricas (guardar nombre y valor de la métrica).\n\nLo anterior debe ser implementado utilizando los siguientes modelos:\n\n- `Dummy` con estrategia estratificada.\n- `LogisticRegression`.\n- `KNeighborsClassifier`.\n- `DecisionTreeClassifier`\n- `SVC`\n- `RandomForestClassifier` \n- `LightGBMClassifier` (del paquete `lightgbm`)\n- `XGBClassifier` (del paquete `xgboost`).\n\n\nLuego, transformando el diccionario de las métricas a un pandas `DataFrame`, ordene según los valores de su métrica de mayor a menor y responda.\n- ¿Hay algún clasificador entrenado mejor que el azar (`Dummy`)?\n- ¿Cuál es el mejor clasificador entrenado?\n- ¿Por qué el mejor clasificador es mejor que los otros?\n- Respecto al tiempo de entrenamiento, con cual cree que sería mejor experimentar (piense en el tiempo que le tomaría pasar el modelo por una grilla de optimización de hiperparámetros).\n\n**Nota**: Puede utilizar un for más una lista con las clases de los modelos mencionados para simplificar el proceso anterior.\n\n\n### 5. Optimización del Modelo [1.5 puntos]\n\n_En esta sección deben mejorar del modelo de clasificación al variar los algoritmos/hiperparámetros que están ocupando._\n\n- Instanciar dos nuevas `Pipeline`, similares a la anterior, pero ahora enfocada en buscar el mejor modelo. Para esto, la pipelines debe utilizar el primer y segundo mejor modelo encontrado en el paso anterior.\n- Usar **`GridSearchCV`** o **`HalvingGridSearchCV`** para tunear hipermarámetros. La primera demorará más que la segunda pero les traerá potencialmente mejores resultados.\n- **Importante**: Recuerden setear la búsqueda para optimizar la métrica seleccionada en los puntos anteriores.\n\nAlgunas ideas para mejorar el rendimiento de sus modelos:\n\n- Agregar técnicas de seleccion de atributos/características. El parámetro de cuántas características se seleccionan debe ser parametrizable y configurado por el optimizador de hiperparámetros.\n- Variar el imputador de datos en caso de usarlo.\n\n#### Bonus\n\n1. **Optuna** [0.5 extras]: Pueden probar también [`OptunaSearchCV`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.OptunaSearchCV.html) de la librería [`Optuna`](https://optuna.org/), la cuál es bastante popular para buscar modelos de redes neuronales.\n2. **Visualización con Optuna** [0.2 extras]: Explore la documentación de visualización de Optuna en el siguiente [link](https://optuna.readthedocs.io/en/stable/reference/visualization/index.html) y realice un análisis sobre el proceso de optimización de hiperparámetros realizado.\n3. **Imabalanced learn** [0.3 extras]: Al ser el problema desbalanceado, pueden probar técnicas para balancear automáticamente el dataset previo a ejecutar el modelo. Para esto, puede probar con los mecanismos implementados en la librería [Imbalanced learn](https://imbalanced-learn.org/). \n4. **Probar pycaret (AutoML) [0.3 extras]**.\n\nAlgunas notas interesantes sobre este proceso:\n\n- No se les pide rendimientos cercanos al 100% de la métrica para concretar exitosamente el proyecto. Por otra parte, celebren cada progreso que obtengan.\n- **Hacer grillas computables**: Si la grilla se va a demorar 1/3 la edad del universo en explorarse completamente, entonces achíquenla a algo que sepan que va a terminar.\n- Aprovechen el procesamiento paralelo (con `njobs`) para acelerar la búsqueda. Sin embargo, si tienen problemas con la memoria RAM, reduzca la cantidad de jobs a algo que su computador/interprete web pueda procesar.\n- La pipelines permiten cachear (guardar temporalmente) etapas cuyo cálculo es redudante, como por ejemplo el escalamiento y la imputación, acelerando así la computación. **Importante:** Para esto, cuando ejecuten `GridSearchCV`, agreguen a la pipeline en el parámetro `memory = \".\"`.\n\n**Al final de este proceso, seleccione el mejor modelo encontrado, prediga el conjunto de prueba y reporte sus resultados.**"},{"cell_type":"markdown","metadata":{"cell_id":"86fadbd406214b998dd528ec52eeecde","deepnote_cell_type":"markdown"},"source":"### 6. Interpretabilidad [1.0 puntos]\n\n_En esta sección, se espera que los estudiantes demuestren su capacidad para explicar cómo sus modelos toman decisiones utilizando los datos. Dentro del análisis de interpretabilidad propuesto para el modelo, deberán ser capaces de:_\n\n- Proponer un método para analizar la interpretabilidad del modelo. Es crucial que puedan justificar por qué el método propuesto es el más adecuado y explicar los alcances que podría tener en su aplicación.\n- Identificar las características más relevantes del modelo. ¿La distribución de importancia es coherente y equitativa entre todas las variables?\n- Analizar 10 observaciones aleatorias utilizando un método específico para verificar la coherencia de las interacciones entre las características.\n- Explorar cómo se relacionan las variables utilizando algún descriptivo de interpretabilidad.\n- ¿Existen variables irrelevantes en el problema modelado?, ¿Cuales son?.\n\nEs fundamental que los estudiantes sean capaces de determinar si su modelo toma decisiones coherentes y evaluar el impacto que podría tener la aplicación de un modelo con esas variables en una población. ¿Es posible que el modelo sea perjudicial o que las estimaciones se basen en decisiones sesgadas?\n\nEn resumen, esta sección busca que los estudiantes apliquen un enfoque crítico para evaluar la interpretabilidad de su modelo, identificar posibles sesgos y analizar las implicaciones de sus decisiones en la población objetivo.\n\n### 7. Concluir [1.0 puntos]\n\n_Aquí deben escribir una breve conclusión del trabajo que hicieron en donde incluyan (pero no se limiten) a responder las siguientes preguntas:_\n\n- ¿Pudieron resolver exitosamente el problema?\n- ¿Son aceptables los resultados obtenidos?\n- ¿En que medida el EDA ayudó a comprender los datos en miras de generar un modelo predictivo?\n\nRespecto a la clasificación:\n\n- ¿Como fue el rendimiento del baseline para la clasificación?\n- ¿Pudieron optimizar el baseline para la clasificación?\n- ¿Que tanto mejoro el baseline de la clasificación con respecto a sus optimizaciones?\n\nFinalmente:\n\n- ¿Estuvieron conformes con sus resultados?\n- ¿Creen que hayan mejores formas de modelar el problema?\n- ¿En general, qué aprendieron del proyecto? ¿Qué no aprendieron y les gustaría haber aprendido?\n\n**OJO** si usted decide responder parte de estas preguntas, debe redactarlas en un formato de informe y no responderlas directamente.\n\n### Otras Instrucciones\n\nRecordar el uso de buenas prácticas de MLOPS como replicabilidad (fijar semillas aleatorias) o el uso del registro de experimentos (con MLFlow). Si bien son opcionales, es altamente recomendado su uso.\n\n### 8. Bonus: Implementación de Kedro y FastAPI [1.5 puntos]\n\n**OPCIONAL**\n\nEn esta sección se les solicita utilizar las últimas tecnologías vistas en el curso para la productivización del proyecto de ciencia de datos, centrándose en la organización y gestión de los flujos de trabajo a través de componentes y pipelines, más el servicio del modelo a través del desarrollo de una API.\n\nPara esto: \n\n1. Genere un proyecto de `Kedro` en donde separe por responsabilidades los nodos/componentes de su proyecto de ciencia de datos en módulos separados. [1.0 puntos]\n2. Genere un servidor basado en `FastAPI` el cuál a través de un método post, reciba un batch de datos y genere predicciones para cada uno de ellos. [0.5 puntos]\n\nLas implementaciones son libres. Es decir, usted decide qué componentes implementar, como usar el catálogo de datos y la parametrización del flujo. Sin embargo, evaluaremos buen uso de los framework, modularización y separación de responsabilidades.\n"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"competencia"},"language_info":{"name":"python","version":"3.9.16","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"orig_nbformat":4,"deepnote_notebook_id":"4a4ad4306407402db0c258f95d5a9abe","deepnote_execution_queue":[]}}